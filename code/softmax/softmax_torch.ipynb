{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d7aa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc95a62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af138264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "\n",
    "# Load CIFAR10 raw (train only) to compute mean image\n",
    "cifar_train = datasets.CIFAR10(root=\"./data\", train=True, download=True)\n",
    "X = np.stack([np.array(img) for img, _ in cifar_train])  # (50000, 32, 32, 3)\n",
    "mean_image = X.mean(axis=0)  # (32, 32, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d2dbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "CIFAR10_MEAN = [0.4914, 0.4822, 0.4465]\n",
    "CIFAR10_STD  = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "    transforms.Lambda(torch.flatten)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dcbfbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Load full training set (50k samples)\n",
    "full_train = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "# Create 45k / 5k split\n",
    "train_size = 45000\n",
    "val_size   = 5000\n",
    "\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "# Load test set separately\n",
    "test_ds = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=200, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=200, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=200, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "078feffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "img, label = train_ds[32]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4e10021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3072, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d694c28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftmaxClassifier(\n",
       "  (linear): Linear(in_features=3072, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SoftmaxClassifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c76261ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device)   # X: (batch, 3072)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stats\n",
    "        running_loss += loss.item() * y.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5af9f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "reg = 1e-4\n",
    "num_epochs = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4417d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    num_epochs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device=DEVICE,\n",
    "):\n",
    "    model = SoftmaxClassifier().to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(\n",
    "            f\"[lr={lr:.1e}, wd={weight_decay:.1e}] \"\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"train loss {train_loss:.4f}, acc {train_acc:.3f} | \"\n",
    "            f\"val loss {val_loss:.4f}, acc {val_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "    return best_val_acc, best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c20a9564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr=1.0e-03, wd=1.0e-04] Epoch 01 | train loss 2.0349, acc 0.274 | val loss 1.9225, acc 0.328\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 02 | train loss 1.8967, acc 0.341 | val loss 1.8658, acc 0.353\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 03 | train loss 1.8548, acc 0.360 | val loss 1.8393, acc 0.365\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 04 | train loss 1.8307, acc 0.371 | val loss 1.8208, acc 0.373\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 05 | train loss 1.8137, acc 0.380 | val loss 1.8125, acc 0.373\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 06 | train loss 1.8012, acc 0.385 | val loss 1.8019, acc 0.379\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 07 | train loss 1.7905, acc 0.389 | val loss 1.7949, acc 0.382\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 08 | train loss 1.7820, acc 0.394 | val loss 1.7873, acc 0.385\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 09 | train loss 1.7748, acc 0.397 | val loss 1.7832, acc 0.388\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 10 | train loss 1.7683, acc 0.398 | val loss 1.7793, acc 0.393\n"
     ]
    }
   ],
   "source": [
    "best_val_acc, best_state = train_model(\n",
    "    lr=learning_rate,\n",
    "    weight_decay=reg,\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81076889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 1.7773, test acc: 0.390\n"
     ]
    }
   ],
   "source": [
    "best_model = SoftmaxClassifier().to(DEVICE)\n",
    "best_model.load_state_dict(best_state)\n",
    "\n",
    "test_loss, test_acc = evaluate(best_model, test_loader, loss_fn, DEVICE)\n",
    "print(f\"\\nTest loss: {test_loss:.4f}, test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "feac105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training with lr=1.0e-03, weight_decay=0.0e+00\n",
      "[lr=1.0e-03, wd=0.0e+00] Epoch 01 | train loss 2.0388, acc 0.275 | val loss 1.9266, acc 0.332\n",
      "[lr=1.0e-03, wd=0.0e+00] Epoch 02 | train loss 1.8960, acc 0.346 | val loss 1.8716, acc 0.359\n",
      "[lr=1.0e-03, wd=0.0e+00] Epoch 03 | train loss 1.8553, acc 0.364 | val loss 1.8447, acc 0.369\n",
      "[lr=1.0e-03, wd=0.0e+00] Epoch 04 | train loss 1.8315, acc 0.374 | val loss 1.8278, acc 0.374\n",
      "[lr=1.0e-03, wd=0.0e+00] Epoch 05 | train loss 1.8152, acc 0.381 | val loss 1.8159, acc 0.379\n",
      "Best val acc for lr=1.0e-03, wd=0.0e+00: 0.379\n",
      "============================================================\n",
      "Training with lr=1.0e-03, weight_decay=1.0e-04\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 01 | train loss 2.0423, acc 0.271 | val loss 1.9266, acc 0.334\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 02 | train loss 1.8989, acc 0.343 | val loss 1.8688, acc 0.357\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 03 | train loss 1.8571, acc 0.362 | val loss 1.8412, acc 0.369\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 04 | train loss 1.8325, acc 0.373 | val loss 1.8247, acc 0.380\n",
      "[lr=1.0e-03, wd=1.0e-04] Epoch 05 | train loss 1.8157, acc 0.382 | val loss 1.8110, acc 0.381\n",
      "Best val acc for lr=1.0e-03, wd=1.0e-04: 0.381\n",
      "============================================================\n",
      "Training with lr=1.0e-03, weight_decay=5.0e-04\n",
      "[lr=1.0e-03, wd=5.0e-04] Epoch 01 | train loss 2.0377, acc 0.272 | val loss 1.9173, acc 0.333\n",
      "[lr=1.0e-03, wd=5.0e-04] Epoch 02 | train loss 1.8941, acc 0.346 | val loss 1.8634, acc 0.362\n",
      "[lr=1.0e-03, wd=5.0e-04] Epoch 03 | train loss 1.8532, acc 0.365 | val loss 1.8381, acc 0.369\n",
      "[lr=1.0e-03, wd=5.0e-04] Epoch 04 | train loss 1.8299, acc 0.377 | val loss 1.8218, acc 0.375\n",
      "[lr=1.0e-03, wd=5.0e-04] Epoch 05 | train loss 1.8133, acc 0.383 | val loss 1.8102, acc 0.379\n",
      "Best val acc for lr=1.0e-03, wd=5.0e-04: 0.379\n",
      "============================================================\n",
      "Training with lr=1.0e-03, weight_decay=1.0e-03\n",
      "[lr=1.0e-03, wd=1.0e-03] Epoch 01 | train loss 2.0267, acc 0.280 | val loss 1.9205, acc 0.327\n",
      "[lr=1.0e-03, wd=1.0e-03] Epoch 02 | train loss 1.8904, acc 0.349 | val loss 1.8670, acc 0.353\n",
      "[lr=1.0e-03, wd=1.0e-03] Epoch 03 | train loss 1.8507, acc 0.368 | val loss 1.8393, acc 0.363\n",
      "[lr=1.0e-03, wd=1.0e-03] Epoch 04 | train loss 1.8275, acc 0.376 | val loss 1.8226, acc 0.377\n",
      "[lr=1.0e-03, wd=1.0e-03] Epoch 05 | train loss 1.8115, acc 0.383 | val loss 1.8119, acc 0.380\n",
      "Best val acc for lr=1.0e-03, wd=1.0e-03: 0.380\n",
      "============================================================\n",
      "Training with lr=3.0e-03, weight_decay=0.0e+00\n",
      "[lr=3.0e-03, wd=0.0e+00] Epoch 01 | train loss 1.9310, acc 0.327 | val loss 1.8502, acc 0.365\n",
      "[lr=3.0e-03, wd=0.0e+00] Epoch 02 | train loss 1.8216, acc 0.377 | val loss 1.8113, acc 0.376\n",
      "[lr=3.0e-03, wd=0.0e+00] Epoch 03 | train loss 1.7900, acc 0.389 | val loss 1.7905, acc 0.390\n",
      "[lr=3.0e-03, wd=0.0e+00] Epoch 04 | train loss 1.7706, acc 0.397 | val loss 1.7844, acc 0.391\n",
      "[lr=3.0e-03, wd=0.0e+00] Epoch 05 | train loss 1.7557, acc 0.401 | val loss 1.7744, acc 0.391\n",
      "Best val acc for lr=3.0e-03, wd=0.0e+00: 0.391\n",
      "============================================================\n",
      "Training with lr=3.0e-03, weight_decay=1.0e-04\n",
      "[lr=3.0e-03, wd=1.0e-04] Epoch 01 | train loss 1.9346, acc 0.323 | val loss 1.8488, acc 0.368\n",
      "[lr=3.0e-03, wd=1.0e-04] Epoch 02 | train loss 1.8231, acc 0.375 | val loss 1.8070, acc 0.386\n",
      "[lr=3.0e-03, wd=1.0e-04] Epoch 03 | train loss 1.7912, acc 0.389 | val loss 1.7896, acc 0.394\n",
      "[lr=3.0e-03, wd=1.0e-04] Epoch 04 | train loss 1.7711, acc 0.397 | val loss 1.7814, acc 0.392\n",
      "[lr=3.0e-03, wd=1.0e-04] Epoch 05 | train loss 1.7572, acc 0.403 | val loss 1.7712, acc 0.392\n",
      "Best val acc for lr=3.0e-03, wd=1.0e-04: 0.394\n",
      "============================================================\n",
      "Training with lr=3.0e-03, weight_decay=5.0e-04\n",
      "[lr=3.0e-03, wd=5.0e-04] Epoch 01 | train loss 1.9370, acc 0.323 | val loss 1.8487, acc 0.367\n",
      "[lr=3.0e-03, wd=5.0e-04] Epoch 02 | train loss 1.8252, acc 0.376 | val loss 1.8084, acc 0.381\n",
      "[lr=3.0e-03, wd=5.0e-04] Epoch 03 | train loss 1.7919, acc 0.389 | val loss 1.7897, acc 0.385\n",
      "[lr=3.0e-03, wd=5.0e-04] Epoch 04 | train loss 1.7722, acc 0.397 | val loss 1.7781, acc 0.396\n",
      "[lr=3.0e-03, wd=5.0e-04] Epoch 05 | train loss 1.7582, acc 0.403 | val loss 1.7669, acc 0.402\n",
      "Best val acc for lr=3.0e-03, wd=5.0e-04: 0.402\n",
      "============================================================\n",
      "Training with lr=3.0e-03, weight_decay=1.0e-03\n",
      "[lr=3.0e-03, wd=1.0e-03] Epoch 01 | train loss 1.9377, acc 0.323 | val loss 1.8412, acc 0.370\n",
      "[lr=3.0e-03, wd=1.0e-03] Epoch 02 | train loss 1.8225, acc 0.375 | val loss 1.8065, acc 0.389\n",
      "[lr=3.0e-03, wd=1.0e-03] Epoch 03 | train loss 1.7908, acc 0.390 | val loss 1.7872, acc 0.394\n",
      "[lr=3.0e-03, wd=1.0e-03] Epoch 04 | train loss 1.7703, acc 0.399 | val loss 1.7767, acc 0.394\n",
      "[lr=3.0e-03, wd=1.0e-03] Epoch 05 | train loss 1.7566, acc 0.405 | val loss 1.7704, acc 0.401\n",
      "Best val acc for lr=3.0e-03, wd=1.0e-03: 0.401\n",
      "============================================================\n",
      "Training with lr=1.0e-02, weight_decay=0.0e+00\n",
      "[lr=1.0e-02, wd=0.0e+00] Epoch 01 | train loss 1.8763, acc 0.348 | val loss 1.8087, acc 0.377\n",
      "[lr=1.0e-02, wd=0.0e+00] Epoch 02 | train loss 1.7810, acc 0.389 | val loss 1.7727, acc 0.396\n",
      "[lr=1.0e-02, wd=0.0e+00] Epoch 03 | train loss 1.7530, acc 0.400 | val loss 1.7646, acc 0.403\n",
      "[lr=1.0e-02, wd=0.0e+00] Epoch 04 | train loss 1.7357, acc 0.406 | val loss 1.7603, acc 0.403\n",
      "[lr=1.0e-02, wd=0.0e+00] Epoch 05 | train loss 1.7202, acc 0.413 | val loss 1.7704, acc 0.395\n",
      "Best val acc for lr=1.0e-02, wd=0.0e+00: 0.403\n",
      "============================================================\n",
      "Training with lr=1.0e-02, weight_decay=1.0e-04\n",
      "[lr=1.0e-02, wd=1.0e-04] Epoch 01 | train loss 1.8765, acc 0.350 | val loss 1.8114, acc 0.377\n",
      "[lr=1.0e-02, wd=1.0e-04] Epoch 02 | train loss 1.7839, acc 0.389 | val loss 1.7839, acc 0.394\n",
      "[lr=1.0e-02, wd=1.0e-04] Epoch 03 | train loss 1.7534, acc 0.399 | val loss 1.7703, acc 0.399\n",
      "[lr=1.0e-02, wd=1.0e-04] Epoch 04 | train loss 1.7367, acc 0.407 | val loss 1.7655, acc 0.387\n",
      "[lr=1.0e-02, wd=1.0e-04] Epoch 05 | train loss 1.7207, acc 0.414 | val loss 1.7546, acc 0.400\n",
      "Best val acc for lr=1.0e-02, wd=1.0e-04: 0.400\n",
      "============================================================\n",
      "Training with lr=1.0e-02, weight_decay=5.0e-04\n",
      "[lr=1.0e-02, wd=5.0e-04] Epoch 01 | train loss 1.8769, acc 0.348 | val loss 1.8210, acc 0.373\n",
      "[lr=1.0e-02, wd=5.0e-04] Epoch 02 | train loss 1.7857, acc 0.387 | val loss 1.7754, acc 0.398\n",
      "[lr=1.0e-02, wd=5.0e-04] Epoch 03 | train loss 1.7553, acc 0.400 | val loss 1.7650, acc 0.400\n",
      "[lr=1.0e-02, wd=5.0e-04] Epoch 04 | train loss 1.7353, acc 0.406 | val loss 1.7579, acc 0.399\n",
      "[lr=1.0e-02, wd=5.0e-04] Epoch 05 | train loss 1.7223, acc 0.413 | val loss 1.7707, acc 0.394\n",
      "Best val acc for lr=1.0e-02, wd=5.0e-04: 0.400\n",
      "============================================================\n",
      "Training with lr=1.0e-02, weight_decay=1.0e-03\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 01 | train loss 1.8753, acc 0.352 | val loss 1.7986, acc 0.380\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 02 | train loss 1.7825, acc 0.389 | val loss 1.7907, acc 0.393\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 03 | train loss 1.7540, acc 0.402 | val loss 1.7735, acc 0.393\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 04 | train loss 1.7346, acc 0.407 | val loss 1.7605, acc 0.398\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 05 | train loss 1.7230, acc 0.412 | val loss 1.7592, acc 0.404\n",
      "Best val acc for lr=1.0e-02, wd=1.0e-03: 0.404\n",
      "\n",
      "Hyperparameter search results:\n",
      "lr=1.0e-02, wd=1.0e-03 -> best val acc = 0.404\n",
      "lr=1.0e-02, wd=0.0e+00 -> best val acc = 0.403\n",
      "lr=3.0e-03, wd=5.0e-04 -> best val acc = 0.402\n",
      "lr=3.0e-03, wd=1.0e-03 -> best val acc = 0.401\n",
      "lr=1.0e-02, wd=1.0e-04 -> best val acc = 0.400\n",
      "lr=1.0e-02, wd=5.0e-04 -> best val acc = 0.400\n",
      "lr=3.0e-03, wd=1.0e-04 -> best val acc = 0.394\n",
      "lr=3.0e-03, wd=0.0e+00 -> best val acc = 0.391\n",
      "lr=1.0e-03, wd=1.0e-04 -> best val acc = 0.381\n",
      "lr=1.0e-03, wd=1.0e-03 -> best val acc = 0.380\n",
      "lr=1.0e-03, wd=5.0e-04 -> best val acc = 0.379\n",
      "lr=1.0e-03, wd=0.0e+00 -> best val acc = 0.379\n",
      "\n",
      "Best overall config:\n",
      "lr=1.0e-02, wd=1.0e-03, val acc=0.404\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "learning_rates = [1e-3, 3e-3, 1e-2]\n",
    "weight_decays  = [0.0, 1e-4, 5e-4, 1e-3]\n",
    "\n",
    "num_epochs = 5  # keep small for tuning; bump later for the best combo\n",
    "\n",
    "results = {}\n",
    "best_overall_acc = 0.0\n",
    "best_overall_cfg = None\n",
    "best_overall_state = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Training with lr={lr:.1e}, weight_decay={wd:.1e}\")\n",
    "        best_val_acc, best_state = train_model(\n",
    "            lr=lr,\n",
    "            weight_decay=wd,\n",
    "            num_epochs=num_epochs,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        results[(lr, wd)] = best_val_acc\n",
    "        print(f\"Best val acc for lr={lr:.1e}, wd={wd:.1e}: {best_val_acc:.3f}\")\n",
    "\n",
    "        if best_val_acc > best_overall_acc:\n",
    "            best_overall_acc = best_val_acc\n",
    "            best_overall_cfg = (lr, wd)\n",
    "            best_overall_state = best_state\n",
    "\n",
    "print(\"\\nHyperparameter search results:\")\n",
    "for (lr, wd), acc in sorted(results.items(), key=lambda x: -x[1]):\n",
    "    print(f\"lr={lr:.1e}, wd={wd:.1e} -> best val acc = {acc:.3f}\")\n",
    "\n",
    "print(\"\\nBest overall config:\")\n",
    "print(f\"lr={best_overall_cfg[0]:.1e}, wd={best_overall_cfg[1]:.1e}, val acc={best_overall_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c222d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr=1.0e-02, wd=1.0e-03] Epoch 01 | train loss 1.8769, acc 0.350 | val loss 1.8028, acc 0.387\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 02 | train loss 1.7829, acc 0.392 | val loss 1.7775, acc 0.394\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 03 | train loss 1.7528, acc 0.402 | val loss 1.7786, acc 0.383\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 04 | train loss 1.7345, acc 0.409 | val loss 1.7568, acc 0.399\n",
      "[lr=1.0e-02, wd=1.0e-03] Epoch 05 | train loss 1.7218, acc 0.412 | val loss 1.7496, acc 0.403\n",
      "\n",
      "Test loss: 1.7445, test acc: 0.401\n"
     ]
    }
   ],
   "source": [
    "best_val_acc, best_state = train_model(\n",
    "    lr=1.0e-02,\n",
    "    weight_decay=1.0e-03,\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    )\n",
    "\n",
    "best_model = SoftmaxClassifier().to(DEVICE)\n",
    "best_model.load_state_dict(best_state)\n",
    "\n",
    "test_loss, test_acc = evaluate(best_model, test_loader, loss_fn, DEVICE)\n",
    "print(f\"\\nTest loss: {test_loss:.4f}, test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc262aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
